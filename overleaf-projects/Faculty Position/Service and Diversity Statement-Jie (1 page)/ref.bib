@article{gao2023coaicoder,
author = {Gao, Jie and Choo, Kenny Tsu Wei and Cao, Junming and Lee, Roy Ka-Wei and Perrault, Simon},
title = {CoAIcoder: Examining the Effectiveness of AI-assisted Human-to-Human Collaboration in Qualitative Analysis},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3617362},
doi = {10.1145/3617362},
abstract = {While AI-assisted individual qualitative analysis has been substantially studied, AI-assisted collaborative qualitative analysis (CQA) – a process that involves multiple researchers working together to interpret data—remains relatively unexplored. After identifying CQA practices and design opportunities through formative interviews, we designed and implemented CoAIcoder, a tool leveraging AI to enhance human-to-human collaboration within CQA through four distinct collaboration methods. With a between-subject design, we evaluated CoAIcoder with 32 pairs of CQA-trained participants across common CQA phases under each collaboration method. Our findings suggest that while using a shared AI model as a mediator among coders could improve CQA efficiency and foster agreement more quickly in the early coding stage, it might affect the final code diversity. We also emphasize the need to consider the independence level when using AI to assist human-to-human collaboration in various CQA scenarios. Lastly, we suggest design implications for future AI-assisted CQA systems.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = nov,
articleno = {6},
numpages = {38},
keywords = {Qualitative coding, collaboration, AI-assisted qualitative analysis, coding quality, AI-assisted human-to-human collaboration}
}


@inproceedings{gao2024collabcoder,
author = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
title = {CollabCoder: A Lower-barrier, Rigorous Workflow for Inductive Collaborative Qualitative Analysis with Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642002},
doi = {10.1145/3613904.3642002},
abstract = {Collaborative Qualitative Analysis (CQA) can enhance qualitative analysis rigor and depth by incorporating varied viewpoints. Nevertheless, ensuring a rigorous CQA procedure itself can be both complex and costly. To lower this bar, we take a theoretical perspective to design a one-stop, end-to-end workflow, CollabCoder, that integrates Large Language Models (LLMs) into key inductive CQA stages. In the independent open coding phase, CollabCoder offers AI-generated code suggestions and records decision-making data. During the iterative discussion phase, it promotes mutual understanding by sharing this data within the coding team and using quantitative metrics to identify coding (dis)agreements, aiding in consensus-building. In the codebook development phase, CollabCoder provides primary code group suggestions, lightening the workload of developing a codebook from scratch. A 16-user evaluation confirmed the effectiveness of CollabCoder, demonstrating its advantages over the existing CQA platform. All related materials of CollabCoder, including code and further extensions, will be included in: https://gaojie058.github.io/CollabCoder/.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {11},
numpages = {29},
keywords = {Collaborative Qualitative Analysis, Grounded Theory, Inductive Qualitative Coding, Large Language Models},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@misc{gao2025mindcoder,
      title={Efficiency with Rigor! A Trustworthy LLM-powered Workflow for Qualitative Data Analysis}, 
      author={Jie Gao and Zhiyao Shu and Shun Yi Yeo and Alok Prakash and Chien-Ming Huang and Mark Dredze and Ziang Xiao},
      year={2025},
      eprint={2501.00775},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2501.00775}, 
}


@misc{gao2025codemap,
      title={Chain of Understanding: Supporting Code Understanding with Large Language Models}, 
      author={Jie Gao and Yue Xue and Xiaofei Xie and SoeMin Thant and Erika Lee},
      year={2025},
      eprint={2504.04553},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2504.04553}, 
}


@inproceedings{zhang2023visar,
author = {Zhang, Zheng and Gao, Jie and Dhaliwal, Ranjodh Singh and Li, Toby Jia-Jun},
title = {VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606800},
doi = {10.1145/3586183.3606800},
abstract = {In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confirmed the usability and effectiveness of VISAR in facilitating the argumentative writing planning process.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {5},
numpages = {30},
keywords = {creativity support, human-AI collaboration, writing support},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{gao2024taxonomy,
author = {Gao, Jie and Gebreegziabher, Simret Araya and Choo, Kenny Tsu Wei and Li, Toby Jia-Jun and Perrault, Simon Tangi and Malone, Thomas W},
title = {A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650786},
doi = {10.1145/3613905.3650786},
abstract = {With ChatGPT’s release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow—planning, facilitating, iterating, and testing—to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the “5W1H” guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {24},
numpages = {11},
keywords = {Human-LLM Interaction, Large Language Models, Taxonomy},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{gao2022difference,
author = {Gao, Jie and Ying, Xiayin and Cao, Junming and Yang, Yifan and Foong, Pin Sym and Perrault, Simon},
title = {Differences of Challenges of Working from Home (WFH) between Weibo and Twitter Users during COVID-19},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519790},
doi = {10.1145/3491101.3519790},
abstract = {People face lots of challenges when working from home (WFH). In this paper, we used both LDA (Latent Dirichlet Allocation) topic modeling and qualitative analysis to analyse WFH related posts on Weibo (N=1093) and Twitter (N=907) during COVID-19. We highlighted unique differences of WFH challenges between two platforms, including long work time, family and food commitment and health concerns on Weibo; casual wearing habits on Twitter. We then provided possible guidelines from a cross-cultural perspective on how to improve the WFH experience based on these differences.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {259},
numpages = {7},
keywords = {LDA topic modeling, Social media, qualitative analysis, user comparison},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}


@inproceedings{marianne2024llms,
author = {Aubin Le Qu\'{e}r\'{e}, Marianne and Schroeder, Hope and Randazzo, Casey and Gao, Jie and Epstein, Ziv and Perrault, Simon Tangi and Mimno, David and Barkhuus, Louise and Li, Hanlin},
title = {LLMs as Research Tools: Applications and Evaluations in HCI Data Work},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3636301},
doi = {10.1145/3613905.3636301},
abstract = {Large language models (LLMs) stand to reshape traditional methods of working with data. While LLMs unlock new and potentially useful ways of interfacing with data, their use in research processes requires methodological and critical evaluation. In this workshop, we seek to gather a community of HCI researchers interested in navigating the responsible integration of LLMs into data work: data collection, processing, and analysis. We aim to create an understanding of how LLMs are being used to work with data in HCI research, and document the early challenges and concerns that have arisen. Together, we will outline a research agenda on using LLMs as research tools to work with data by defining the open empirical and ethical evaluation questions and thus contribute to setting norms in the community. We believe CHI to be the ideal place to address these questions due to the methodologically diverse researcher attendees, the prevalence of HCI research on human interaction with new computing and data paradigms, and the community’s sense of ethics and care. Insights from this forum can contribute to other research communities grappling with related questions.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {479},
numpages = {7},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{yeo2024help,
author = {Yeo, ShunYi and Lim, Gionnieve and Gao, Jie and Zhang, Weiyu and Perrault, Simon Tangi},
title = {Help Me Reflect: Leveraging Self-Reflection Interface Nudges to Enhance Deliberativeness on Online Deliberation Platforms},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642530},
doi = {10.1145/3613904.3642530},
abstract = {The deliberative potential of online platforms has been widely examined. However, little is known about how various interface-based reflection nudges impact the quality of deliberation. This paper presents two user studies with 12 and 120 participants, respectively, to investigate the impacts of different reflective nudges on the quality of deliberation. In the first study, we examined five distinct reflective nudges: persona, temporal prompts, analogies and metaphors, cultural prompts and storytelling. Persona, temporal prompts, and storytelling emerged as the preferred nudges for implementation on online deliberation platforms. In the second study, we assess the impacts of these preferred reflectors more thoroughly. Results revealed a significant positive impact of these reflectors on deliberative quality. Specifically, persona promotes a deliberative environment for balanced and opinionated viewpoints while temporal prompts promote more individualised viewpoints. Our findings suggest that the choice of reflectors can significantly influence the dynamics and shape the nature of online discussions.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {806},
numpages = {32},
keywords = {civic engagement, deliberation, deliberative quality, deliberativeness, internal reflection, large language model, nudges, online deliberation, persona, public discussions, reflection, reflexivity, self-reflection, storytelling, temporal prompts},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

