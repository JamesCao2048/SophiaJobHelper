#!/usr/bin/env python3
"""
course_catalog_scraper.py -- Step 1 è¾…åŠ©ï¼šè¯¾ç¨‹ä½“ç³»æŠ“å–

æŠ“å–ç›®æ ‡é™¢ç³»è¯¾ç¨‹é¡µé¢ï¼Œæå–è¯¾ç¨‹åˆ—è¡¨ï¼Œ
å†™å…¥ faculty_data.json çš„ department_courses å­—æ®µã€‚

äº”å±‚ fallback ç­–ç•¥ï¼š
  Layer 1:   curl + browser UA
  Layer 1.5: Jina Reader (https://r.jina.ai/)
  Layer 2:   Tavily Extract API
  Layer 2.5: Wayback Machine
  Layer 3:   Tavily Search API

ç”¨æ³•:
  python course_catalog_scraper.py \
    --url "https://www.monash.edu/it/dsai/courses" \
    --output overseas_pipeline/output/monash_university/faculty_data.json

  python course_catalog_scraper.py \
    --url "https://www.monash.edu/it/dsai/courses" \
    --dry-run
"""

import argparse
import json
import re
import sys
from datetime import datetime
from pathlib import Path

try:
    from web_fetch_utils import fetch_with_fallback, log
except ImportError:
    import os
    sys.path.insert(0, os.path.dirname(__file__))
    from web_fetch_utils import fetch_with_fallback, log

HCI_COURSE_KEYWORDS = [
    "hci", "human-computer", "interaction design", "user experience",
    "ux", "usability", "accessibility", "human factors",
    "interface", "user interface", "human-ai", "conversational",
    "visualization", "information design", "human centered",
]


def is_hci_course(name: str) -> bool:
    low = name.lower()
    return any(kw in low for kw in HCI_COURSE_KEYWORDS)


def extract_courses_heuristic(text: str) -> list[dict]:
    """
    å¯å‘å¼æå–è¯¾ç¨‹ç¼–å· + åç§°ã€‚
    Agent ä¼šåœ¨æ­¤åŸºç¡€ä¸Šå®¡æŸ¥å’Œè¡¥å……ã€‚
    """
    courses = []
    # å¸¸è§è¯¾ç¨‹ç¼–å·æ ¼å¼ï¼šFIT5145, CS101, COMP3702, INFO4112 ç­‰
    pattern = re.compile(
        r"\b([A-Z]{2,6}\s*\d{3,5}[A-Z]?)\s*[:\-\u2013]?\s*([^\n\r,;]{10,80})",
        re.MULTILINE,
    )
    for match in pattern.finditer(text):
        code = match.group(1).strip().replace(" ", "")
        name = match.group(2).strip().rstrip(".,;")
        if len(name) < 5:
            continue
        digits = re.search(r"\d+", code)
        level = "unknown"
        if digits:
            n = int(digits.group())
            if n >= 5000:
                level = "postgrad"
            elif n >= 3000:
                level = "undergrad_advanced"
            else:
                level = "undergrad"

        courses.append({
            "code": code,
            "name": name,
            "level": level,
            "hci_relevant": is_hci_course(name),
        })

    # å»é‡ï¼ˆæŒ‰ codeï¼‰
    seen: set[str] = set()
    unique = []
    for c in courses:
        if c["code"] not in seen:
            seen.add(c["code"])
            unique.append(c)

    return unique


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Course catalog scraper with five-layer fallback"
    )
    parser.add_argument("--url", required=True, help="Course catalog URL")
    parser.add_argument("--output", help="Path to faculty_data.json to update")
    parser.add_argument("--school", default="", help="School name (for search fallback)")
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    content, layer = fetch_with_fallback(args.url, args.school)

    if not content:
        print(
            "ERROR: All five layers failed. Manual paste required.", file=sys.stderr
        )
        sys.exit(1)

    courses = extract_courses_heuristic(content)
    hci_count = sum(1 for c in courses if c["hci_relevant"])
    log(f"Extracted {len(courses)} courses ({hci_count} HCI-relevant), via {layer}")

    result = {
        "department_courses": courses,
        "course_catalog_url": args.url,
        "course_catalog_scrape_date": datetime.now().strftime("%Y-%m-%d"),
        "course_fetch_layer": layer,
        "course_count": len(courses),
    }

    print("\n=== Course Catalog ===")
    for c in courses[:10]:
        tag = "ğŸ”µ" if c["hci_relevant"] else "  "
        print(f"  {tag} [{c['level']}] {c['code']}: {c['name']}")
    if len(courses) > 10:
        print(f"  ... and {len(courses) - 10} more")
    print(f"\nLayer used: {layer}")

    if args.dry_run:
        print("\n[dry-run] Not writing.")
        return

    if not args.output:
        print("WARNING: --output not specified. Printing JSON only.")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        return

    output_path = Path(args.output)

    # ä¿å­˜åŸå§‹å†…å®¹åˆ° raw/ ç›®å½•ï¼Œä¾› agent åœ¨æ­£åˆ™æå–ä¸ºç©ºæ—¶ç›´æ¥åˆ†æ
    raw_dir = output_path.parent / "raw"
    raw_dir.mkdir(parents=True, exist_ok=True)
    raw_path = raw_dir / "course_catalog_raw.md"
    with open(raw_path, "w", encoding="utf-8") as f:
        f.write(f"<!-- source: {args.url} | layer: {layer} | date: {datetime.now().strftime('%Y-%m-%d')} -->\n\n")
        f.write(content)
    log(f"Raw content saved to {raw_path} ({len(content)} chars)")

    if output_path.exists():
        with open(output_path, encoding="utf-8") as f:
            faculty_data = json.load(f)
    else:
        faculty_data = {}

    faculty_data.update(result)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(faculty_data, f, indent=2, ensure_ascii=False)

    print(f"\nâœ“ Written to {output_path}")
    if len(courses) == 0:
        print(f"âš  è¯¾ç¨‹æå–ä¸ºç©ºï¼ˆ{layer} å†…å®¹å·²ä¿å­˜åˆ° {raw_path}ï¼‰")
        print("  Agent åº”åœ¨ Step 1 step 10 å®¡æŸ¥ä¸­ç›´æ¥è¯»å– raw æ–‡ä»¶è¯†åˆ«è¯¾ç¨‹")


if __name__ == "__main__":
    main()
