#!/usr/bin/env python3
"""
course_catalog_scraper.py -- Step 1 è¾…åŠ©ï¼šè¯¾ç¨‹ä½“ç³»æŠ“å–

æŠ“å–ç›®æ ‡é™¢ç³»è¯¾ç¨‹é¡µé¢ï¼Œæå–è¯¾ç¨‹åˆ—è¡¨ï¼Œ
å†™å…¥ faculty_data.json çš„ department_courses å­—æ®µã€‚

äº”å±‚ fallback ç­–ç•¥ï¼š
  Layer 1:   curl + browser UA
  Layer 1.5: Jina Reader (https://r.jina.ai/)
  Layer 2:   Tavily Extract API
  Layer 2.5: Wayback Machine
  Layer 3:   Tavily Search API

ç”¨æ³•:
  python course_catalog_scraper.py \
    --url "https://www.monash.edu/it/dsai/courses" \
    --output overseas_pipeline/output/monash_university/faculty_data.json

  python course_catalog_scraper.py \
    --url "https://www.monash.edu/it/dsai/courses" \
    --dry-run
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path

try:
    import requests
except ImportError:
    print("ERROR: pip install requests", file=sys.stderr)
    sys.exit(1)

TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY", "")
TIMEOUT = 20

HCI_COURSE_KEYWORDS = [
    "hci", "human-computer", "interaction design", "user experience",
    "ux", "usability", "accessibility", "human factors",
    "interface", "user interface", "human-ai", "conversational",
    "visualization", "information design", "human centered",
]

BROWSER_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
}


def log(msg: str) -> None:
    print(f"[course_scraper] {msg}", flush=True)


def is_blocked(text: str) -> bool:
    signals = [
        "just a moment", "cf_chl_opt", "challenge-platform",
        "enable javascript", "403 forbidden", "access denied",
    ]
    low = text.lower()
    return any(s in low for s in signals) or len(text) < 300


def layer1_curl(url: str) -> str | None:
    log(f"Layer 1: curl {url}")
    try:
        resp = requests.get(url, headers=BROWSER_HEADERS, timeout=TIMEOUT)
        if is_blocked(resp.text):
            log("  âœ— Blocked (Cloudflare/WAF)")
            return None
        log(f"  âœ“ {len(resp.text)} chars")
        return resp.text
    except Exception as e:
        log(f"  âœ— {e}")
        return None


def layer1_5_jina(url: str) -> str | None:
    log("Layer 1.5: Jina Reader")
    try:
        resp = requests.get(
            f"https://r.jina.ai/{url}",
            headers={"User-Agent": BROWSER_HEADERS["User-Agent"]},
            timeout=TIMEOUT,
        )
        if len(resp.text) < 500 or "error" in resp.text[:200].lower():
            log("  âœ— Jina returned short/error response")
            return None
        log(f"  âœ“ {len(resp.text)} chars")
        return resp.text
    except Exception as e:
        log(f"  âœ— {e}")
        return None


def layer2_tavily_extract(url: str) -> str | None:
    if not TAVILY_API_KEY:
        log("  âš  TAVILY_API_KEY not set, skipping Layer 2")
        return None
    log("Layer 2: Tavily Extract")
    try:
        resp = requests.post(
            "https://api.tavily.com/extract",
            headers={
                "Authorization": f"Bearer {TAVILY_API_KEY}",
                "Content-Type": "application/json",
            },
            json={"urls": [url]},
            timeout=TIMEOUT,
        )
        data = resp.json()
        results = data.get("results", [])
        if not results:
            log("  âœ— No results")
            return None
        content = results[0].get("raw_content", "")
        log(f"  âœ“ {len(content)} chars")
        return content
    except Exception as e:
        log(f"  âœ— {e}")
        return None


def layer2_5_wayback(url: str) -> str | None:
    log("Layer 2.5: Wayback Machine")
    for year in ["2024", "2023"]:
        wayback_url = f"https://web.archive.org/web/{year}/{url}"
        try:
            resp = requests.get(wayback_url, headers=BROWSER_HEADERS, timeout=TIMEOUT)
            if not is_blocked(resp.text) and len(resp.text) > 500:
                log(f"  âœ“ {len(resp.text)} chars (year={year})")
                return resp.text
        except Exception as e:
            log(f"  âœ— year={year}: {e}")
    return None


def layer3_tavily_search(url: str, school: str = "") -> str | None:
    if not TAVILY_API_KEY:
        log("  âš  TAVILY_API_KEY not set, skipping Layer 3")
        return None
    domain = url.split("/")[2] if "//" in url else url
    query = f"site:{domain} course catalog {school} courses list"
    log(f"Layer 3: Tavily Search '{query}'")
    try:
        resp = requests.post(
            "https://api.tavily.com/search",
            headers={
                "Authorization": f"Bearer {TAVILY_API_KEY}",
                "Content-Type": "application/json",
            },
            json={"query": query, "max_results": 5, "include_raw_content": True},
            timeout=TIMEOUT,
        )
        data = resp.json()
        results = data.get("results", [])
        if not results:
            log("  âœ— No results")
            return None
        combined = "\n\n".join(r.get("content", "") for r in results)
        log(f"  âœ“ {len(combined)} chars from {len(results)} results")
        return combined
    except Exception as e:
        log(f"  âœ— {e}")
        return None


def fetch_with_fallback(url: str, school: str = "") -> tuple[str | None, str]:
    """äº”å±‚ fallbackï¼Œè¿”å› (content, layer_used)"""
    content = layer1_curl(url)
    if content:
        return content, "layer1_curl"

    content = layer1_5_jina(url)
    if content:
        return content, "layer1.5_jina"

    content = layer2_tavily_extract(url)
    if content:
        return content, "layer2_tavily_extract"

    content = layer2_5_wayback(url)
    if content:
        return content, "layer2.5_wayback"

    content = layer3_tavily_search(url, school)
    if content:
        return content, "layer3_tavily_search"

    return None, "all_failed"


def is_hci_course(name: str) -> bool:
    low = name.lower()
    return any(kw in low for kw in HCI_COURSE_KEYWORDS)


def extract_courses_heuristic(text: str) -> list[dict]:
    """
    å¯å‘å¼æå–è¯¾ç¨‹ç¼–å· + åç§°ã€‚
    Agent ä¼šåœ¨æ­¤åŸºç¡€ä¸Šå®¡æŸ¥å’Œè¡¥å……ã€‚
    """
    courses = []
    # å¸¸è§è¯¾ç¨‹ç¼–å·æ ¼å¼ï¼šFIT5145, CS101, COMP3702, INFO4112 ç­‰
    pattern = re.compile(
        r"\b([A-Z]{2,6}\s*\d{3,5}[A-Z]?)\s*[:\-\u2013]?\s*([^\n\r,;]{10,80})",
        re.MULTILINE,
    )
    for match in pattern.finditer(text):
        code = match.group(1).strip().replace(" ", "")
        name = match.group(2).strip().rstrip(".,;")
        if len(name) < 5:
            continue
        digits = re.search(r"\d+", code)
        level = "unknown"
        if digits:
            n = int(digits.group())
            if n >= 5000:
                level = "postgrad"
            elif n >= 3000:
                level = "undergrad_advanced"
            else:
                level = "undergrad"

        courses.append({
            "code": code,
            "name": name,
            "level": level,
            "hci_relevant": is_hci_course(name),
        })

    # å»é‡ï¼ˆæŒ‰ codeï¼‰
    seen: set[str] = set()
    unique = []
    for c in courses:
        if c["code"] not in seen:
            seen.add(c["code"])
            unique.append(c)

    return unique


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Course catalog scraper with five-layer fallback"
    )
    parser.add_argument("--url", required=True, help="Course catalog URL")
    parser.add_argument("--output", help="Path to faculty_data.json to update")
    parser.add_argument("--school", default="", help="School name (for search fallback)")
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    content, layer = fetch_with_fallback(args.url, args.school)

    if not content:
        print(
            "ERROR: All five layers failed. Manual paste required.", file=sys.stderr
        )
        sys.exit(1)

    courses = extract_courses_heuristic(content)
    hci_count = sum(1 for c in courses if c["hci_relevant"])
    log(f"Extracted {len(courses)} courses ({hci_count} HCI-relevant), via {layer}")

    result = {
        "department_courses": courses,
        "course_catalog_url": args.url,
        "course_catalog_scrape_date": datetime.now().strftime("%Y-%m-%d"),
        "course_fetch_layer": layer,
        "course_count": len(courses),
    }

    print("\n=== Course Catalog ===")
    for c in courses[:10]:
        tag = "ğŸ”µ" if c["hci_relevant"] else "  "
        print(f"  {tag} [{c['level']}] {c['code']}: {c['name']}")
    if len(courses) > 10:
        print(f"  ... and {len(courses) - 10} more")
    print(f"\nLayer used: {layer}")

    if args.dry_run:
        print("\n[dry-run] Not writing.")
        return

    if not args.output:
        print("WARNING: --output not specified. Printing JSON only.")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        return

    output_path = Path(args.output)

    # ä¿å­˜åŸå§‹å†…å®¹åˆ° raw/ ç›®å½•ï¼Œä¾› agent åœ¨æ­£åˆ™æå–ä¸ºç©ºæ—¶ç›´æ¥åˆ†æ
    raw_dir = output_path.parent / "raw"
    raw_dir.mkdir(parents=True, exist_ok=True)
    raw_path = raw_dir / "course_catalog_raw.md"
    with open(raw_path, "w", encoding="utf-8") as f:
        f.write(f"<!-- source: {args.url} | layer: {layer} | date: {datetime.now().strftime('%Y-%m-%d')} -->\n\n")
        f.write(content)
    log(f"Raw content saved to {raw_path} ({len(content)} chars)")

    if output_path.exists():
        with open(output_path, encoding="utf-8") as f:
            faculty_data = json.load(f)
    else:
        faculty_data = {}

    faculty_data.update(result)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(faculty_data, f, indent=2, ensure_ascii=False)

    print(f"\nâœ“ Written to {output_path}")
    if len(courses) == 0:
        print(f"âš  è¯¾ç¨‹æå–ä¸ºç©ºï¼ˆ{layer} å†…å®¹å·²ä¿å­˜åˆ° {raw_path}ï¼‰")
        print("  Agent åº”åœ¨ Step 1 step 10 å®¡æŸ¥ä¸­ç›´æ¥è¯»å– raw æ–‡ä»¶è¯†åˆ«è¯¾ç¨‹")


if __name__ == "__main__":
    main()
