#!/usr/bin/env python3
"""
å…¨çƒæ•™èŒæ¯æ—¥ç›‘æ§ç³»ç»Ÿ
===========================
æ¯æ—¥è‡ªåŠ¨æŠ“å–å¤šä¸ªå­¦æœ¯æ‹›è˜å¹³å°çš„ HCI / AI / CS æ–¹å‘æ•™èŒä¿¡æ¯ï¼Œ
ç­›é€‰åæ¨è 2-3 ä¸ªæœ€åŒ¹é…çš„èŒä½ï¼Œå¹¶å†™å…¥ Excel è¿½è¸ªè¡¨ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    python3 faculty_monitor.py          # è¿è¡ŒæŠ“å–å¹¶è¾“å‡ºä»Šæ—¥æ¨è
    python3 faculty_monitor.py --all    # è¾“å‡ºæ‰€æœ‰æŠ“åˆ°çš„èŒä½ï¼ˆä¸ä»…é™æ¨èï¼‰

ä¾èµ–ï¼šrequests, bs4, openpyxl (å‡ä¸ºå¸¸è§åŒ…)
å¦‚æœæ²¡æœ‰ feedparserï¼Œç”¨å†…ç½® xml.etree è§£æ RSSã€‚
"""

import os, sys, re, json, hashlib
from datetime import datetime, timedelta
from pathlib import Path
import xml.etree.ElementTree as ET

import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook, load_workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter

# â”€â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORDS_HCI = [
    "human-computer interaction", "HCI", "human-AI", "human AI interaction",
    "interactive systems", "user experience", "UX research",
    "computer-supported cooperative work", "CSCW", "social computing",
    "human-centered AI", "human centered computing", "information science",
]
KEYWORDS_AI = [
    "artificial intelligence", "machine learning", "natural language processing",
    "NLP", "deep learning", "computer vision", "data science",
    "AI", "large language model", "LLM", "generative AI",
]
KEYWORDS_ALL = KEYWORDS_HCI + KEYWORDS_AI

POSITION_KEYWORDS = [
    "assistant professor", "associate professor", "professor",
    "tenure", "tenure-track", "faculty", "lecturer", "senior lecturer",
    "reader", "chair", "endowed",
    # Non-tenure track å­¦æœ¯æ•™èŒï¼ˆç¨³å®šå²—ä½ï¼‰
    "teaching professor", "research professor", "adjunct professor",
    "clinical professor", "instructor", "principal lecturer",
]

POSITION_KEYWORDS_NON_TENURE = [
    "lecturer", "senior lecturer", "teaching professor", "research professor",
    "adjunct professor", "clinical professor",
    "instructor", "principal lecturer",
    "non-tenure", "non tenure", "teaching track",
]

EXCLUDE_KEYWORDS = [
    "postdoc", "post-doc", "postdoctoral", "PhD student", "research assistant",
    "lab manager", "technician",
    # ä¸´æ—¶æ€§/ä¸šç•Œå¯¼å‘èŒä½
    "visiting professor", "visiting assistant professor", "research fellow",
    "professor of practice", "industry professor",
]

SCRIPT_DIR = Path(__file__).parent
EXCEL_OUTPUT = SCRIPT_DIR / "æ•™èŒè¿½è¸ªè¡¨.xlsx"
SEEN_FILE = SCRIPT_DIR / ".seen_jobs.json"
LOG_FILE = SCRIPT_DIR / "monitor_log.txt"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36"
}
TIMEOUT = 20


# â”€â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def log(msg):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(line + "\n")


def job_id(title, url):
    raw = f"{title.strip().lower()}|{url.strip().lower()}"
    return hashlib.md5(raw.encode()).hexdigest()[:12]


def load_seen():
    if SEEN_FILE.exists():
        return json.loads(SEEN_FILE.read_text(encoding="utf-8"))
    return {}


def save_seen(seen):
    SEEN_FILE.write_text(json.dumps(seen, ensure_ascii=False, indent=2), encoding="utf-8")


def matches_keywords(text, keywords):
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in keywords)


def is_faculty_position(text):
    return matches_keywords(text, POSITION_KEYWORDS)


def is_excluded(text):
    return matches_keywords(text, EXCLUDE_KEYWORDS)


def relevance_score(title, description=""):
    combined = f"{title} {description}".lower()
    score = 0
    for kw in KEYWORDS_HCI:
        if kw.lower() in combined:
            score += 3  # HCI æ–¹å‘åŠ æƒ
    for kw in KEYWORDS_AI:
        if kw.lower() in combined:
            score += 2
    if is_faculty_position(combined):
        score += 5
    if is_excluded(combined):
        score -= 10
    # Tenure-track åŠ åˆ†
    if "tenure-track" in combined or "tenure track" in combined:
        score += 3
    if "assistant professor" in combined:
        score += 2
    # Non-tenure track åŒæ ·åŠ åˆ†ï¼ˆä¸å†æ­§è§†ï¼‰
    for kw in POSITION_KEYWORDS_NON_TENURE:
        if kw.lower() in combined:
            score += 2
            break  # åªåŠ ä¸€æ¬¡
    return score


def safe_get(url, **kwargs):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT, **kwargs)
        resp.raise_for_status()
        return resp
    except Exception as e:
        log(f"  âš  è¯·æ±‚å¤±è´¥ {url}: {e}")
        return None


def parse_rss(xml_text):
    """è§£æ RSS/Atom feedï¼Œè¿”å› [(title, link, description, pub_date)]"""
    items = []
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError:
        return items

    ns = {"atom": "http://www.w3.org/2005/Atom"}

    # RSS 2.0
    for item in root.iter("item"):
        title = (item.findtext("title") or "").strip()
        link = (item.findtext("link") or "").strip()
        desc = (item.findtext("description") or "").strip()
        pub = (item.findtext("pubDate") or "").strip()
        if title and link:
            items.append((title, link, desc, pub))

    # Atom
    if not items:
        for entry in root.iter("{http://www.w3.org/2005/Atom}entry"):
            title = (entry.findtext("{http://www.w3.org/2005/Atom}title") or "").strip()
            link_el = entry.find("{http://www.w3.org/2005/Atom}link")
            link = link_el.get("href", "") if link_el is not None else ""
            desc = (entry.findtext("{http://www.w3.org/2005/Atom}summary") or "").strip()
            pub = (entry.findtext("{http://www.w3.org/2005/Atom}updated") or "").strip()
            if title and link:
                items.append((title, link, desc, pub))

    return items


# â”€â”€â”€ å„å¹³å°æŠ“å–æ¨¡å— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_jobs_ac_uk():
    """jobs.ac.uk - è‹±å›½æ•™èŒ RSS"""
    log("ğŸ“¡ æŠ“å– jobs.ac.uk (RSS)...")
    jobs = []
    feeds = [
        "http://www.jobs.ac.uk/jobs/computer-sciences?format=rss",
        "http://www.jobs.ac.uk/jobs/artificial-intelligence?format=rss",
        "http://www.jobs.ac.uk/jobs/information-systems?format=rss",
    ]
    for feed_url in feeds:
        resp = safe_get(feed_url)
        if not resp:
            continue
        for title, link, desc, pub in parse_rss(resp.text):
            combined = f"{title} {desc}"
            if matches_keywords(combined, KEYWORDS_ALL) and not is_excluded(title):
                jobs.append({
                    "title": title, "url": link, "source": "jobs.ac.uk",
                    "region": "è‹±å›½/æ¬§æ´²", "description": desc[:300],
                    "date_found": pub or datetime.now().strftime("%Y-%m-%d"),
                    "score": relevance_score(title, desc),
                })
    log(f"  âœ“ jobs.ac.uk æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_higheredjobs():
    """HigherEdJobs - åŒ—ç¾é«˜æ ¡ RSS (å¯èƒ½è¢«åçˆ¬æ‹¦æˆª)"""
    log("ğŸ“¡ æŠ“å– HigherEdJobs (RSS)...")
    jobs = []
    # Category 102 = Computer Science, 144 = Information Systems and Technology
    feeds = [
        "https://www.higheredjobs.com/search/rss.cfm?JobCat=102",
        "https://www.higheredjobs.com/search/rss.cfm?JobCat=144",
    ]
    for feed_url in feeds:
        resp = safe_get(feed_url)
        if not resp:
            continue
        # æ£€æŸ¥è¿”å›çš„æ˜¯å¦æ˜¯çœŸæ­£çš„ RSS XMLï¼ˆè€Œéåçˆ¬ HTML é¡µé¢ï¼‰
        content = resp.text.strip()
        if content.startswith("<!DOCTYPE") or content.startswith("<html"):
            log(f"  âš  HigherEdJobs è¿”å› HTML è€Œé RSSï¼ˆå¯èƒ½è¢«åçˆ¬æ‹¦æˆªï¼‰")
            continue
        for title, link, desc, pub in parse_rss(content):
            combined = f"{title} {desc}"
            if (matches_keywords(combined, KEYWORDS_ALL) or is_faculty_position(title)) and not is_excluded(title):
                jobs.append({
                    "title": title, "url": link, "source": "HigherEdJobs",
                    "region": "åŒ—ç¾", "description": desc[:300],
                    "date_found": pub or datetime.now().strftime("%Y-%m-%d"),
                    "score": relevance_score(title, desc),
                })
    log(f"  âœ“ HigherEdJobs æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_cra():
    """CRA Career Center - JSON API"""
    log("ğŸ“¡ æŠ“å– CRA Career Center (API)...")
    jobs = []
    api_headers = dict(HEADERS)
    api_headers["Accept"] = "application/json"
    search_queries = ["professor", "faculty", "HCI", "AI", "computer science"]
    seen_ids = set()
    for query in search_queries:
        try:
            resp = requests.get(
                f"https://careercenter.cra.org/api/v1/jobs?keywords={query}&resultsPerPage=25",
                headers=api_headers, timeout=TIMEOUT,
            )
            resp.raise_for_status()
            data = resp.json()
            for item in data.get("data", []):
                jid = item.get("id")
                if jid in seen_ids:
                    continue
                seen_ids.add(jid)
                title = item.get("title", "")
                url = item.get("url", "")
                desc = item.get("shortDescription", "")
                location = item.get("location", "")
                company = item.get("company", {})
                org = company.get("name", "") if isinstance(company, dict) else ""
                combined = f"{title} {desc}"
                if not is_excluded(title):
                    jobs.append({
                        "title": title, "url": url, "source": "CRA",
                        "region": f"åŒ—ç¾ - {location}" if location else "åŒ—ç¾",
                        "description": f"{org} - {desc[:200]}" if org else desc[:200],
                        "date_found": item.get("posted_date", datetime.now().strftime("%Y-%m-%d")),
                        "score": relevance_score(title, desc),
                    })
        except Exception as e:
            log(f"  âš  CRA API æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
    log(f"  âœ“ CRA æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_euraxess():
    """EURAXESS - æ¬§æ´²å­¦æœ¯èŒä½"""
    log("ğŸ“¡ æŠ“å– EURAXESS...")
    jobs = []
    search_queries = [
        "HCI artificial intelligence human-computer",
        "professor computer science",
        "machine learning faculty",
    ]
    seen_urls = set()
    for kw in search_queries:
        resp = safe_get("https://euraxess.ec.europa.eu/jobs/search", params={"keywords": kw})
        if not resp:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select("article.ecl-content-item"):
            title_el = item.select_one("h3 a, h2 a")
            if not title_el:
                continue
            title = title_el.get_text(strip=True)
            link = title_el.get("href", "")
            if link and not link.startswith("http"):
                link = "https://euraxess.ec.europa.eu" + link
            if link in seen_urls:
                continue
            seen_urls.add(link)
            # è·å–æœºæ„ä¿¡æ¯
            text_parts = item.get_text(" ", strip=True)
            if not is_excluded(title):
                jobs.append({
                    "title": title, "url": link, "source": "EURAXESS",
                    "region": "æ¬§æ´²", "description": text_parts[:300],
                    "date_found": datetime.now().strftime("%Y-%m-%d"),
                    "score": relevance_score(title, text_parts),
                })
    log(f"  âœ“ EURAXESS æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_chronicle():
    """Chronicle of Higher Education - åŒ—ç¾æ•™èŒ"""
    log("ğŸ“¡ æŠ“å– Chronicle of Higher Education...")
    jobs = []
    search_queries = [
        "computer+science+professor",
        "HCI+faculty",
        "artificial+intelligence+professor",
    ]
    for query in search_queries:
        url = f"https://jobs.chronicle.com/searchjobs/?Keywords={query}&radialtown=&LocationId=&RadialLocation=0&CountryCode=&JobType=&SalaryFrom=&SalaryTo=&SalaryType=&PositionType="
        resp = safe_get(url)
        if not resp:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select("div.js-clickable-area, article, div.lister__item"):
            title_el = item.select_one("h3 a, h2 a, a.js-clickable-area-link")
            if not title_el:
                continue
            title = title_el.get_text(strip=True)
            link = title_el.get("href", "")
            if link and not link.startswith("http"):
                link = "https://jobs.chronicle.com" + link
            loc_el = item.select_one(".lister__meta-item, .location, .job-location")
            location = loc_el.get_text(strip=True) if loc_el else ""
            if not is_excluded(title):
                jobs.append({
                    "title": title, "url": link, "source": "Chronicle",
                    "region": location or "åŒ—ç¾", "description": "",
                    "date_found": datetime.now().strftime("%Y-%m-%d"),
                    "score": relevance_score(title),
                })
    log(f"  âœ“ Chronicle æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_the_unijobs():
    """Times Higher Education Unijobs"""
    log("ğŸ“¡ æŠ“å– THE Unijobs...")
    jobs = []
    urls = [
        "https://www.timeshighereducation.com/unijobs/listings/computer-science/",
    ]
    for url in urls:
        resp = safe_get(url)
        if not resp:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select("li.lister__item"):
            title_el = item.select_one("h3.lister__header a, h3 a, h2 a")
            if not title_el:
                continue
            title = title_el.get_text(strip=True)
            if title.startswith("View details"):
                continue
            link = title_el.get("href", "").strip()
            if link and not link.startswith("http"):
                link = "https://www.timeshighereducation.com" + link
            combined = title
            if matches_keywords(combined, KEYWORDS_ALL) and not is_excluded(title):
                jobs.append({
                    "title": title, "url": link, "source": "THE Unijobs",
                    "region": "å…¨çƒ", "description": "",
                    "date_found": datetime.now().strftime("%Y-%m-%d"),
                    "score": relevance_score(title),
                })
    log(f"  âœ“ THE Unijobs æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_inside_highered():
    """Inside Higher Ed - åŒ—ç¾/å…¨çƒæ•™èŒ"""
    log("ğŸ“¡ æŠ“å– Inside Higher Ed...")
    jobs = []
    search_queries = [
        "computer+science+professor",
        "HCI+faculty",
        "artificial+intelligence",
    ]
    for query in search_queries:
        url = f"https://careers.insidehighered.com/searchjobs/?Keywords={query}&radialtown=&LocationId=&RadialLocation=0"
        resp = safe_get(url)
        if not resp:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        for item in soup.select("div.js-clickable-area, article, div.lister__item"):
            title_el = item.select_one("h3 a, h2 a, a.js-clickable-area-link")
            if not title_el:
                continue
            title = title_el.get_text(strip=True)
            link = title_el.get("href", "")
            if link and not link.startswith("http"):
                link = "https://careers.insidehighered.com" + link
            loc_el = item.select_one(".lister__meta-item, .location, .job-location")
            location = loc_el.get_text(strip=True) if loc_el else ""
            if not is_excluded(title):
                jobs.append({
                    "title": title, "url": link, "source": "Inside Higher Ed",
                    "region": location or "åŒ—ç¾", "description": "",
                    "date_found": datetime.now().strftime("%Y-%m-%d"),
                    "score": relevance_score(title),
                })
    log(f"  âœ“ Inside Higher Ed æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_jrecin():
    """JREC-IN Portal - æ—¥æœ¬å­¦æœ¯èŒä½ï¼ˆè‹±æ–‡æ£€ç´¢ï¼‰"""
    log("ğŸ“¡ æŠ“å– JREC-IN (æ—¥æœ¬)...")
    jobs = []
    # JREC-IN æ”¯æŒè‹±æ–‡å…³é”®è¯æœç´¢ï¼Œå­—æ®µ bg1=01 è¡¨ç¤ºç†å·¥ç§‘
    search_queries = [
        {"keyword": "HCI human-computer interaction", "bg1": "01"},
        {"keyword": "artificial intelligence professor", "bg1": "01"},
        {"keyword": "computer science faculty", "bg1": "01"},
    ]
    seen_urls = set()
    for params in search_queries:
        resp = safe_get(
            "https://jrecin.jst.go.jp/seek/SeekJorSearch",
            params={"fn": "1", "ln": "1", "bg1": params["bg1"],
                    "sm1": "01", "keyword": params["keyword"], "lang": "1"},
        )
        if not resp:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        # JREC-IN èŒä½åˆ—è¡¨ç»“æ„
        for item in soup.select("table.tbl_listtype1 tr"):
            cols = item.find_all("td")
            if len(cols) < 3:
                continue
            title_el = cols[1].find("a") if len(cols) > 1 else None
            if not title_el:
                continue
            title = title_el.get_text(strip=True)
            link = title_el.get("href", "")
            if link and not link.startswith("http"):
                link = "https://jrecin.jst.go.jp" + link
            if link in seen_urls:
                continue
            seen_urls.add(link)
            org = cols[0].get_text(strip=True) if cols else ""
            combined = f"{title} {org}"
            if matches_keywords(combined, KEYWORDS_ALL) and not is_excluded(title):
                jobs.append({
                    "title": title, "url": link, "source": "JREC-IN",
                    "region": "æ—¥æœ¬", "description": org[:200],
                    "date_found": datetime.now().strftime("%Y-%m-%d"),
                    "score": relevance_score(title, org),
                })
    log(f"  âœ“ JREC-IN æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_academicpositions():
    """AcademicPositions.com - å…¨çƒå­¦æœ¯èŒä½ï¼ˆå«äºšæ´²/ä¸­ä¸œ/æ¬§æ´²ï¼‰"""
    log("ğŸ“¡ æŠ“å– AcademicPositions.com (å…¨çƒ)...")
    jobs = []
    # å°è¯• RSS feedï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    rss_urls = [
        "https://academicpositions.com/jobs/rss?field=computer-science",
        "https://academicpositions.com/jobs/rss?field=information-technology",
    ]
    for rss_url in rss_urls:
        resp = safe_get(rss_url)
        if not resp:
            continue
        content = resp.text.strip()
        if content.startswith("<") and "<item" in content:
            for title, link, desc, pub in parse_rss(content):
                combined = f"{title} {desc}"
                if not is_excluded(title):
                    jobs.append({
                        "title": title, "url": link, "source": "AcademicPositions",
                        "region": "å…¨çƒ", "description": desc[:300],
                        "date_found": pub or datetime.now().strftime("%Y-%m-%d"),
                        "score": relevance_score(title, desc),
                    })
    # å¦‚æœ RSS ä¸å¯ç”¨ï¼Œå›é€€åˆ°ç½‘é¡µæŠ“å–
    if not jobs:
        search_urls = [
            "https://academicpositions.com/jobs?field=computer-science&type=professor",
            "https://academicpositions.com/jobs?field=computer-science&type=lecturer",
        ]
        seen_urls = set()
        for url in search_urls:
            resp = safe_get(url)
            if not resp:
                continue
            soup = BeautifulSoup(resp.text, "html.parser")
            for item in soup.select("article.job-ad, div.job-listing, div[class*='job']"):
                title_el = item.select_one("h2 a, h3 a, a[class*='title']") or item.find("a")
                if not title_el:
                    continue
                title = title_el.get_text(strip=True)
                link = title_el.get("href", "")
                if link and not link.startswith("http"):
                    link = "https://academicpositions.com" + link
                if link in seen_urls or not link:
                    continue
                seen_urls.add(link)
                desc_el = item.select_one("p, div[class*='desc'], div[class*='summary']")
                desc = desc_el.get_text(strip=True)[:300] if desc_el else ""
                if not is_excluded(title):
                    jobs.append({
                        "title": title, "url": link, "source": "AcademicPositions",
                        "region": "å…¨çƒ", "description": desc,
                        "date_found": datetime.now().strftime("%Y-%m-%d"),
                        "score": relevance_score(title, desc),
                    })
    log(f"  âœ“ AcademicPositions æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


def fetch_seek_au():
    """Seek.com.au - æ¾³æ´²/æ–°è¥¿å…°å­¦æœ¯èŒä½"""
    log("ğŸ“¡ æŠ“å– Seek.com.au (æ¾³æ´²/æ–°è¥¿å…°)...")
    jobs = []
    search_queries = [
        "HCI professor",
        "computer science lecturer",
        "artificial intelligence faculty",
    ]
    seen_urls = set()
    for query in search_queries:
        # Seek çš„æœç´¢ URL æ ¼å¼
        url = f"https://www.seek.com.au/{query.replace(' ', '-')}-jobs/in-All-Australia"
        resp = safe_get(url)
        if not resp:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        # Seek ä½¿ç”¨ data-automation å±æ€§æ ‡è®°èŒä½å…ƒç´ 
        for item in soup.select("article[data-automation='normalJob'], div[data-automation='jobCard']"):
            title_el = item.select_one("a[data-automation='jobTitle'], h3 a, h2 a")
            if not title_el:
                continue
            title = title_el.get_text(strip=True)
            link = title_el.get("href", "")
            if link and not link.startswith("http"):
                link = "https://www.seek.com.au" + link
            if link in seen_urls or not link:
                continue
            seen_urls.add(link)
            org_el = item.select_one("a[data-automation='jobCompany'], span[class*='company']")
            org = org_el.get_text(strip=True) if org_el else ""
            loc_el = item.select_one("span[data-automation='jobCardLocation'], span[class*='location']")
            location = loc_el.get_text(strip=True) if loc_el else "æ¾³æ´²"
            combined = f"{title} {org}"
            if (matches_keywords(combined, KEYWORDS_ALL) or is_faculty_position(title)) and not is_excluded(title):
                jobs.append({
                    "title": title, "url": link, "source": "Seek.com.au",
                    "region": f"æ¾³æ´² - {location}" if location else "æ¾³æ´²",
                    "description": org[:200],
                    "date_found": datetime.now().strftime("%Y-%m-%d"),
                    "score": relevance_score(title, org),
                })
    log(f"  âœ“ Seek.com.au æ‰¾åˆ° {len(jobs)} ä¸ªç›¸å…³èŒä½")
    return jobs


# â”€â”€â”€ æ ¸å¿ƒé€»è¾‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_all_jobs():
    all_jobs = []
    fetchers = [
        fetch_jobs_ac_uk,
        fetch_higheredjobs,
        fetch_cra,
        fetch_euraxess,
        fetch_chronicle,
        fetch_the_unijobs,
        fetch_inside_highered,
        # æ–°å¢ï¼šäºšå¤ª + å…¨çƒå¹³å°
        fetch_jrecin,
        fetch_academicpositions,
        fetch_seek_au,
    ]
    for fetcher in fetchers:
        try:
            jobs = fetcher()
            all_jobs.extend(jobs)
        except Exception as e:
            log(f"  âŒ {fetcher.__name__} å‡ºé”™: {e}")
    return all_jobs


def deduplicate(jobs):
    seen_ids = set()
    unique = []
    for j in jobs:
        jid = job_id(j["title"], j.get("url", ""))
        if jid not in seen_ids:
            seen_ids.add(jid)
            unique.append(j)
    return unique


def filter_new_jobs(jobs, seen):
    new_jobs = []
    for j in jobs:
        jid = job_id(j["title"], j.get("url", ""))
        if jid not in seen:
            new_jobs.append(j)
            seen[jid] = {
                "title": j["title"],
                "first_seen": datetime.now().strftime("%Y-%m-%d"),
            }
    return new_jobs


def recommend_top_jobs(jobs, n=3):
    ranked = sorted(jobs, key=lambda j: j.get("score", 0), reverse=True)
    return ranked[:n]


# â”€â”€â”€ Excel è¾“å‡º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SHEET_HEADERS = ["æ—¥æœŸ", "æ¨èæ’å", "èŒä½åç§°", "å­¦æ ¡/æœºæ„", "åœ°åŒº", "æ¥æºå¹³å°", "é“¾æ¥", "åŒ¹é…åº¦è¯„åˆ†", "ç”³è¯·çŠ¶æ€", "å¤‡æ³¨"]

HEADER_FILL = PatternFill("solid", fgColor="1F4E79")
HEADER_FONT = Font(name="Arial", bold=True, color="FFFFFF", size=11)
DATA_FONT = Font(name="Arial", size=10)
LINK_FONT = Font(name="Arial", size=10, color="0563C1", underline="single")
BORDER = Border(
    left=Side(style="thin", color="D9D9D9"),
    right=Side(style="thin", color="D9D9D9"),
    top=Side(style="thin", color="D9D9D9"),
    bottom=Side(style="thin", color="D9D9D9"),
)
STATUS_COLORS = {
    "æœªç”³è¯·": PatternFill("solid", fgColor="FFF2CC"),
    "å·²ç”³è¯·": PatternFill("solid", fgColor="D9EAD3"),
    "é¢è¯•ä¸­": PatternFill("solid", fgColor="D0E0F0"),
    "å·²æ‹’": PatternFill("solid", fgColor="F4CCCC"),
}
COL_WIDTHS = [12, 10, 45, 30, 14, 18, 50, 12, 12, 25]


def init_excel():
    if EXCEL_OUTPUT.exists():
        return load_workbook(str(EXCEL_OUTPUT))
    wb = Workbook()
    ws = wb.active
    ws.title = "æ¯æ—¥æ¨è"
    for col_idx, header in enumerate(SHEET_HEADERS, 1):
        cell = ws.cell(row=1, column=col_idx, value=header)
        cell.font = HEADER_FONT
        cell.fill = HEADER_FILL
        cell.alignment = Alignment(horizontal="center", vertical="center")
        cell.border = BORDER
    for col_idx, width in enumerate(COL_WIDTHS, 1):
        ws.column_dimensions[get_column_letter(col_idx)].width = width
    ws.freeze_panes = "A2"
    ws.auto_filter.ref = f"A1:{get_column_letter(len(SHEET_HEADERS))}1"

    # ç¬¬äºŒä¸ª sheetï¼šå…¨éƒ¨æŠ“å–è®°å½•
    ws2 = wb.create_sheet("å…¨éƒ¨èŒä½")
    all_headers = ["æ—¥æœŸ", "èŒä½åç§°", "æ¥æºå¹³å°", "åœ°åŒº", "é“¾æ¥", "åŒ¹é…åº¦è¯„åˆ†", "æè¿°æ‘˜è¦"]
    for col_idx, header in enumerate(all_headers, 1):
        cell = ws2.cell(row=1, column=col_idx, value=header)
        cell.font = HEADER_FONT
        cell.fill = HEADER_FILL
        cell.alignment = Alignment(horizontal="center", vertical="center")
    col_widths_2 = [12, 45, 18, 14, 50, 12, 60]
    for col_idx, width in enumerate(col_widths_2, 1):
        ws2.column_dimensions[get_column_letter(col_idx)].width = width
    ws2.freeze_panes = "A2"

    # ç¬¬ä¸‰ä¸ª sheetï¼šå¹³å°æ¸…å•ï¼ˆå‚è€ƒï¼‰
    ws3 = wb.create_sheet("ç›‘æ§å¹³å°")
    platform_headers = ["å¹³å°", "URL", "åœ°åŒº", "ç±»å‹", "æ˜Ÿçº§"]
    platforms = [
        ["jobs.ac.uk", "https://www.jobs.ac.uk/", "è‹±å›½/æ¬§æ´²", "RSS", "â˜…â˜…â˜…â˜…â˜…"],
        ["HigherEdJobs", "https://www.higheredjobs.com/", "åŒ—ç¾", "RSS", "â˜…â˜…â˜…â˜…"],
        ["CRA Career Center", "https://careercenter.cra.org/", "åŒ—ç¾", "API", "â˜…â˜…â˜…â˜…â˜…"],
        ["EURAXESS", "https://euraxess.ec.europa.eu/", "æ¬§æ´²", "ç½‘é¡µ", "â˜…â˜…â˜…â˜…"],
        ["Chronicle", "https://jobs.chronicle.com/", "åŒ—ç¾", "ç½‘é¡µ", "â˜…â˜…â˜…â˜…"],
        ["THE Unijobs", "https://www.timeshighereducation.com/unijobs/", "å…¨çƒ", "ç½‘é¡µ", "â˜…â˜…â˜…â˜…â˜…"],
        ["Inside Higher Ed", "https://careers.insidehighered.com/", "åŒ—ç¾/å…¨çƒ", "ç½‘é¡µ", "â˜…â˜…â˜…â˜…"],
        ["JREC-IN", "https://jrecin.jst.go.jp/", "æ—¥æœ¬", "ç½‘é¡µ", "â˜…â˜…â˜…â˜…â˜…"],
        ["AcademicPositions", "https://academicpositions.com/", "å…¨çƒ(å«äºšæ´²/ä¸­ä¸œ)", "RSS/ç½‘é¡µ", "â˜…â˜…â˜…â˜…"],
        ["Seek.com.au", "https://www.seek.com.au/", "æ¾³æ´²/æ–°è¥¿å…°", "ç½‘é¡µ", "â˜…â˜…â˜…â˜…"],
        ["AJO (æ‰‹åŠ¨)", "https://academicjobsonline.org/ajo/cs", "å…¨çƒ", "æ‰‹åŠ¨", "â˜…â˜…â˜…â˜…â˜…"],
        ["SIGCHI Jobs", "https://sigchi.org/get-involved/jobs/", "å…¨çƒ", "æ‰‹åŠ¨", "â˜…â˜…â˜…â˜…â˜…"],
    ]
    for col_idx, h in enumerate(platform_headers, 1):
        cell = ws3.cell(row=1, column=col_idx, value=h)
        cell.font = HEADER_FONT
        cell.fill = HEADER_FILL
    for r, row_data in enumerate(platforms, 2):
        for c, val in enumerate(row_data, 1):
            ws3.cell(row=r, column=c, value=val).font = DATA_FONT

    wb.save(str(EXCEL_OUTPUT))
    log(f"âœ… åˆ›å»ºæ–°çš„è¿½è¸ªè¡¨: {EXCEL_OUTPUT}")
    return wb


def write_recommendations(wb, recommended_jobs, all_new_jobs):
    today = datetime.now().strftime("%Y-%m-%d")
    ws = wb["æ¯æ—¥æ¨è"]
    start_row = ws.max_row + 1

    for rank, job in enumerate(recommended_jobs, 1):
        row_data = [
            today,
            f"Top {rank}",
            job["title"],
            "",  # å­¦æ ¡/æœºæ„ - å¯ä» description ä¸­æå–
            job.get("region", ""),
            job["source"],
            job.get("url", ""),
            job.get("score", 0),
            "æœªç”³è¯·",
            job.get("description", "")[:80],
        ]
        for col_idx, val in enumerate(row_data, 1):
            cell = ws.cell(row=start_row + rank - 1, column=col_idx, value=val)
            cell.font = DATA_FONT
            cell.border = BORDER
            if col_idx == 7 and val:  # é“¾æ¥åˆ—
                cell.font = LINK_FONT
                cell.hyperlink = val
            if col_idx == 9:  # ç”³è¯·çŠ¶æ€
                cell.fill = STATUS_COLORS.get(val, PatternFill())

    # å†™å…¥å…¨éƒ¨èŒä½ sheet
    ws2 = wb["å…¨éƒ¨èŒä½"]
    start_row2 = ws2.max_row + 1
    for idx, job in enumerate(all_new_jobs):
        row_data = [
            today,
            job["title"],
            job["source"],
            job.get("region", ""),
            job.get("url", ""),
            job.get("score", 0),
            job.get("description", "")[:120],
        ]
        for col_idx, val in enumerate(row_data, 1):
            cell = ws2.cell(row=start_row2 + idx, column=col_idx, value=val)
            cell.font = DATA_FONT

    wb.save(str(EXCEL_OUTPUT))


def print_recommendations(jobs):
    today = datetime.now().strftime("%Y-%m-%d")
    print(f"\n{'='*70}")
    print(f"ğŸ“‹ {today} ä»Šæ—¥æ•™èŒæ¨èï¼ˆTop {len(jobs)}ï¼‰")
    print(f"{'='*70}")
    for i, job in enumerate(jobs, 1):
        print(f"\nğŸ† æ¨è #{i}  (åŒ¹é…åº¦: {job.get('score', 0)})")
        print(f"   èŒä½: {job['title']}")
        print(f"   åœ°åŒº: {job.get('region', 'æœªçŸ¥')}")
        print(f"   æ¥æº: {job['source']}")
        print(f"   é“¾æ¥: {job.get('url', 'N/A')}")
        if job.get("description"):
            print(f"   æ‘˜è¦: {job['description'][:100]}")
    print(f"\n{'='*70}")
    print(f"ğŸ’¡ æç¤ºï¼šä¹Ÿåˆ«å¿˜äº†æ‰‹åŠ¨æ£€æŸ¥ä»¥ä¸‹å¹³å°ï¼š")
    print(f"   â€¢ AJO: https://academicjobsonline.org/ajo/cs")
    print(f"   â€¢ SIGCHI Jobs: https://sigchi.org/get-involved/jobs/")
    print(f"   â€¢ CS Faculty Hiring Wiki: æœç´¢ 'CS faculty hiring 2025-2026 wiki'")
    print(f"{'='*70}\n")


# â”€â”€â”€ ä¸»ç¨‹åº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    show_all = "--all" in sys.argv
    log("ğŸš€ å¼€å§‹æ¯æ—¥æ•™èŒç›‘æ§...")

    # 1. æŠ“å–æ‰€æœ‰å¹³å°
    all_jobs = fetch_all_jobs()
    all_jobs = deduplicate(all_jobs)
    log(f"ğŸ“Š å…±æŠ“å–åˆ° {len(all_jobs)} ä¸ªä¸é‡å¤èŒä½")

    # 2. è¿‡æ»¤æ–°èŒä½
    seen = load_seen()
    new_jobs = filter_new_jobs(all_jobs, seen)
    save_seen(seen)
    log(f"ğŸ†• å…¶ä¸­æ–°èŒä½ {len(new_jobs)} ä¸ª")

    # 3. æ¨è Top 3
    if new_jobs:
        recommended = recommend_top_jobs(new_jobs, n=3)
    else:
        log("æ²¡æœ‰æ–°èŒä½ï¼Œä»æ‰€æœ‰èŒä½ä¸­æ¨è...")
        recommended = recommend_top_jobs(all_jobs, n=3)

    # 4. å†™å…¥ Excel
    wb = init_excel()
    jobs_to_write = new_jobs if new_jobs else all_jobs
    write_recommendations(wb, recommended, jobs_to_write)
    log(f"ğŸ“ å·²å†™å…¥ Excel: {EXCEL_OUTPUT}")

    # 5. æ‰“å°æ¨è
    print_recommendations(recommended)

    # 6. å†™å…¥è¿½è¸ªæ•°æ®åº“
    try:
        import sys as _sys
        import os as _os
        _sys.path.insert(0, _os.path.join(_os.path.dirname(__file__), ".."))
        from tracking.tracking_db import ApplicationTracker
        _tracker = ApplicationTracker()
        _added = 0
        _last_id = None
        for _job in recommended:
            _last_id = _tracker.add_job(
                school=_job.get("title", "")[:80],
                position=_job.get("title", ""),
                region=_job.get("region"),
                job_url=_job.get("url"),
                source="faculty_monitor",
                monitor_score=_job.get("score"),
            )
            _added += 1
        if _added:
            log(f"ğŸ“Š å·²å°† {_added} ä¸ªæ¨èèŒä½å†™å…¥è¿½è¸ªæ•°æ®åº“")
    except Exception as _e:
        log(f"âš ï¸  è¿½è¸ªæ•°æ®åº“å†™å…¥å¤±è´¥ï¼ˆä¸å½±å“ä¸»åŠŸèƒ½ï¼‰: {_e}")

    if show_all:
        print(f"\nğŸ“„ æ‰€æœ‰æŠ“å–åˆ°çš„èŒä½ ({len(all_jobs)}):")
        for j in sorted(all_jobs, key=lambda x: x.get("score", 0), reverse=True):
            print(f"  [{j['score']:>3}] [{j['source']:<20}] {j['title'][:60]}")

    log("âœ… æ¯æ—¥ç›‘æ§å®Œæˆï¼")


if __name__ == "__main__":
    main()
